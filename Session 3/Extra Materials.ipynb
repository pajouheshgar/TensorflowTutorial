{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing previous sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "import numpy as np\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = tf.compat.as_bytes(\"<stripped %d bytes>\"%size)\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "  \n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:800px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, batch_size=32):\n",
    "        self.b = batch_size\n",
    "        housing_data = fetch_california_housing()\n",
    "        self.X = housing_data.data\n",
    "        self.Y = np.reshape(housing_data.target, [-1, 1])\n",
    "        self.Y = np.log(self.Y)\n",
    "        self.X = self.X / np.std(self.X, axis=0)\n",
    "        self.n = len(self.X)\n",
    "        self.X_train, self.Y_train = self.X[: self.n // 2], self.Y[: self.n // 2]\n",
    "        self.X_val, self.Y_val = self.X[self.n // 2:], self.Y[self.n // 2:]\n",
    "        self.n_train, self.n_val = len(self.X_train), len(self.X_val)\n",
    "        self.tp, self.vp = 0, 0\n",
    "        \n",
    "    \n",
    "    def get_train_batch(self):\n",
    "        if self.tp >= self.n_train:\n",
    "            self.tp = 0\n",
    "        xb, yb = self.X_train[self.tp: self.tp + self.b], self.Y_train[self.tp: self.tp + self.b]\n",
    "        self.tp += self.b\n",
    "        return xb, yb\n",
    "    \n",
    "    def get_val_batch(self):\n",
    "        if self.vp >= self.n_train:\n",
    "            self.vp = 0\n",
    "        xb, yb = self.X_val[self.vp: self.vp + self.b], self.Y_val[self.vp: self.vp + self.b]\n",
    "        self.vp += self.b\n",
    "        return xb, yb\n",
    "    \n",
    "dataloader = DataLoader(batch_size=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, name='Model', dataloader=None):\n",
    "        self.name = name\n",
    "        self.dataloader = dataloader\n",
    "        \n",
    "        self.save_path = os.path.join(\"SavedModels/\", self.name)\n",
    "        try:\n",
    "            os.mkdir(self.save_path)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        init_time = datetime.datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "        self.log_path = os.path.join(\"log/\" + self.name + \"-run-\" + init_time)\n",
    "        \n",
    "        self.create_graph()\n",
    "        self.ses = tf.Session()\n",
    "        \n",
    "    \n",
    "    def create_graph(self):\n",
    "        tf.reset_default_graph()\n",
    "        tf.set_random_seed(42)\n",
    "        with tf.variable_scope(self.name):\n",
    "            with tf.variable_scope(\"Placeholders\"):\n",
    "                self.x_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, 8]) # dataloader must provide the shape\n",
    "                self.y_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "            \n",
    "            with tf.variable_scope(\"Inference\"):\n",
    "                h1 = tf.layers.dense(inputs=self.x_placeholder, units=16, activation=tf.nn.sigmoid, name='layer1')\n",
    "                h2 = tf.layers.dense(inputs=h1, units=8, activation=tf.nn.sigmoid, name='layer2')\n",
    "                self.y_hat = tf.layers.dense(inputs=h2, units=1, name='y_hat')\n",
    "                \n",
    "            with tf.variable_scope(\"Optimization\"):\n",
    "                self.loss = tf.losses.mean_squared_error(labels=self.y_placeholder, predictions=self.y_hat)\n",
    "                self.global_step = tf.train.get_or_create_global_step()\n",
    "                learning_rate = tf.train.exponential_decay(learning_rate=1e-4, # 1e-3\n",
    "                                                           decay_steps=1000,\n",
    "                                                           decay_rate=0.5,\n",
    "                                                           global_step=self.global_step,\n",
    "                                                           name='learning_rate')\n",
    "                optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "                self.train_operation = optimizer.minimize(self.loss, global_step=self.global_step)\n",
    "                \n",
    "            with tf.variable_scope(\"Init_save\"):\n",
    "                self.variable_initializer = tf.global_variables_initializer()\n",
    "                self.saver = tf.train.Saver()\n",
    "                \n",
    "            with tf.variable_scope(\"Summary\"):\n",
    "                self.train_summary_writer = tf.summary.FileWriter(self.log_path + \"train\", tf.get_default_graph())\n",
    "                self.validation_summary_writer = tf.summary.FileWriter(self.log_path + \"validation\")\n",
    "                self.loss_summary = tf.summary.scalar(name=\"Mean_Squared_Loss\", tensor=self.loss)\n",
    "                \n",
    "                \n",
    "    def train(self, init=True, steps=10000):\n",
    "        if init:\n",
    "            self.ses.run(self.variable_initializer)\n",
    "        step = 0\n",
    "        while step < steps:\n",
    "            x_train, y_train = self.dataloader.get_train_batch()\n",
    "            x_val, y_val = self.dataloader.get_val_batch()\n",
    "            val_loss_summary = self.ses.run(self.loss_summary, feed_dict={self.x_placeholder: x_val, self.y_placeholder: y_val})\n",
    "            train_loss_summary, step, _ = self.ses.run([self.loss_summary, self.global_step, self.train_operation],\n",
    "                                               feed_dict={self.x_placeholder: x_train, self.y_placeholder: y_train})\n",
    "            self.train_summary_writer.add_summary(train_loss_summary, step)\n",
    "            self.validation_summary_writer.add_summary(val_loss_summary, step)\n",
    "            if step % 1000 == 0:\n",
    "                print(\"Step = \", step)\n",
    "                \n",
    "        self.train_summary_writer.close()\n",
    "        self.validation_summary_writer.close()\n",
    "                \n",
    "    \n",
    "    def save(self):\n",
    "        self.saver.save(self.ses, save_path=self.save_path + \"/\" + self.name + '.ckpt')\n",
    "    \n",
    "    def load(self):\n",
    "        self.saver.restore(self.ses, save_path=self.save_path + \"/\" + self.name + '.ckpt')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step =  0\n",
      "Step =  1000\n",
      "Step =  2000\n",
      "Step =  3000\n",
      "Step =  4000\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(batch_size=64)\n",
    "M1 = Model(name=\"M1\", dataloader=dataloader)\n",
    "# show_graph(tf.get_default_graph())\n",
    "M1.train(init=True, steps=4000)\n",
    "# M1.save()\n",
    "# M1.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on Mnist (More Summaries, Changing DataLoader, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gzip, numpy\n",
    "import pickle\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, batch_size=32):\n",
    "        self.b = batch_size\n",
    "        # Load the dataset\n",
    "        with gzip.open('Dataset/mnist.pkl.gz', 'rb') as f:\n",
    "            u = pickle._Unpickler(f)\n",
    "            u.encoding = 'latin1'\n",
    "            train_set, val_set, _ = u.load()\n",
    "        \n",
    "        self.X_train, self.Y_train = train_set\n",
    "        self.X_val, self.Y_val = val_set\n",
    "        self.X_train = np.reshape(self.X_train, [-1, 28, 28, 1])\n",
    "        self.X_val = np.reshape(self.X_val, [-1, 28, 28, 1])\n",
    "\n",
    "        \n",
    "        self.n_train, self.n_val = len(self.X_train), len(self.X_val)\n",
    "        self.tp, self.vp = 0, 0\n",
    "        \n",
    "    \n",
    "    def get_train_batch(self):\n",
    "        if self.tp >= self.n_train - 1:\n",
    "            self.tp = 0\n",
    "        xb, yb = self.X_train[self.tp: self.tp + self.b], self.Y_train[self.tp: self.tp + self.b]\n",
    "        self.tp += self.b\n",
    "        return xb, yb\n",
    "    \n",
    "    def get_val_batch(self):\n",
    "        if self.vp >= self.n_val - 1:\n",
    "            self.vp = 0\n",
    "        xb, yb = self.X_val[self.vp: self.vp + self.b], self.Y_val[self.vp: self.vp + self.b]\n",
    "        self.vp += self.b\n",
    "        return xb, yb\n",
    "    \n",
    "dataloader = DataLoader(batch_size=5)\n",
    "dataloader.get_train_batch()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, name='Model', dataloader=None):\n",
    "        self.name = name\n",
    "        self.dataloader = dataloader\n",
    "        \n",
    "        self.save_path = os.path.join(\"SavedModels/\", self.name)\n",
    "        try:\n",
    "            os.mkdir(self.save_path)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        init_time = datetime.datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "        self.log_path = os.path.join(\"log/\" + self.name + \"-run-\" + init_time)\n",
    "        \n",
    "        self.create_graph()\n",
    "        self.ses = tf.Session()\n",
    "        \n",
    "    \n",
    "    def create_graph(self):\n",
    "        tf.reset_default_graph()\n",
    "        tf.set_random_seed(42)\n",
    "        with tf.variable_scope(self.name):\n",
    "            with tf.variable_scope(\"Placeholders\"):\n",
    "                self.x_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28, 1]) # dataloader must provide the shape\n",
    "                self.y_placeholder = tf.placeholder(dtype=tf.int64, shape=[None])\n",
    "            \n",
    "            with tf.variable_scope(\"Inference\"):\n",
    "                x_flattened = tf.layers.flatten(self.x_placeholder, name='x_flattened')\n",
    "                h1 = tf.layers.dense(inputs=x_flattened, units=128, activation=tf.nn.tanh, name='layer1')\n",
    "                self.logits = tf.layers.dense(inputs=h1, units=10, name='y_hat')\n",
    "                self.y_hat_probs = tf.nn.softmax(self.logits, name='y_hat_probs')\n",
    "                \n",
    "            with tf.variable_scope(\"Optimization\"):\n",
    "                batch_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y_placeholder,\n",
    "                                                                            logits=self.logits,\n",
    "                                                                            name='batch_loss')\n",
    "                self.loss = tf.reduce_mean(batch_loss, name='loss')\n",
    "                batch_accuracy = tf.cast(\n",
    "                    tf.equal(\n",
    "                        tf.argmax(self.y_hat_probs, axis=1),\n",
    "                        self.y_placeholder\n",
    "                    ), dtype=tf.float32)\n",
    "                self.accuracy = tf.reduce_mean(batch_accuracy)\n",
    "                self.global_step = tf.train.get_or_create_global_step()\n",
    "                learning_rate = tf.train.exponential_decay(learning_rate=1e-2, # 1e-3\n",
    "                                                           decay_steps=2000,\n",
    "                                                           decay_rate=0.5,\n",
    "                                                           global_step=self.global_step,\n",
    "                                                           name='learning_rate')\n",
    "                optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "                self.train_operation = optimizer.minimize(self.loss, global_step=self.global_step)\n",
    "                \n",
    "                ################### $ ###################\n",
    "                layer1_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name + \"/Inference/layer1/\")\n",
    "                print(layer1_variables)\n",
    "                layer1_gradients = tf.gradients(self.loss, layer1_variables)\n",
    "                layer1_gradient_norm = tf.norm(layer1_gradients[0], name='layer1_gradient_norm')\n",
    "                ################### $ ###################\n",
    "                \n",
    "            with tf.variable_scope(\"Init_save\"):\n",
    "                self.variable_initializer = tf.global_variables_initializer()\n",
    "                self.saver = tf.train.Saver()\n",
    "                \n",
    "            with tf.variable_scope(\"Summary\"):\n",
    "                self.loss_summary = tf.summary.scalar(name=\"Cross_Entropy_Loss\", tensor=self.loss)\n",
    "                self.accuracy_summary = tf.summary.scalar(name=\"Accuracy\", tensor=self.accuracy)\n",
    "                self.eval_summaries = tf.summary.merge([self.loss_summary, self.accuracy_summary])\n",
    "                \n",
    "                ################### $ ###################\n",
    "                self.layer1_gradient_norm_summary = tf.summary.scalar(name=\"layer1_gradient_norm\", \n",
    "                                                                      tensor=layer1_gradient_norm)\n",
    "                self.layer1_kernel_histogram_summary = tf.summary.histogram(name='layer1_kernel_histogram', \n",
    "                                                                            values=layer1_variables[0])\n",
    "                self.learning_rate_summary = tf.summary.scalar(name='learning_rate', tensor=learning_rate)\n",
    "                \n",
    "                self.input_image_summary = tf.summary.image(name='input_image', tensor=self.x_placeholder, max_outputs=3)\n",
    "                neuron_activity = tf.reshape(tf.transpose(layer1_variables[0]), [128, 28, 28, 1], name='neuron_activity')\n",
    "                self.neuron_activity_summary = tf.summary.image(name='neuron_activity',\n",
    "                                                                tensor=neuron_activity,\n",
    "                                                                max_outputs=3)\n",
    "                self.merged_summaries = tf.summary.merge_all()\n",
    "                ################### $ ###################\n",
    "                \n",
    "                self.train_summary_writer = tf.summary.FileWriter(self.log_path + \"train\", tf.get_default_graph())\n",
    "                self.validation_summary_writer = tf.summary.FileWriter(self.log_path + \"validation\")\n",
    "    \n",
    "                \n",
    "                \n",
    "                \n",
    "    def train(self, init=True, steps=10000):\n",
    "        if init:\n",
    "            self.ses.run(self.variable_initializer)\n",
    "        step = 0\n",
    "        while step < steps:\n",
    "            x_train, y_train = self.dataloader.get_train_batch()\n",
    "            x_val, y_val = self.dataloader.get_val_batch()\n",
    "            val_loss_summary = self.ses.run(self.eval_summaries, feed_dict={self.x_placeholder: x_val, self.y_placeholder: y_val})\n",
    "            train_summary, step, _ = self.ses.run([self.merged_summaries, self.global_step, self.train_operation],\n",
    "                                               feed_dict={self.x_placeholder: x_train, self.y_placeholder: y_train})\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                self.train_summary_writer.add_summary(train_summary, step)\n",
    "                self.validation_summary_writer.add_summary(val_loss_summary, step)\n",
    "                ################### $ ###################\n",
    "#                 self.train_summary_writer.flush()\n",
    "    #             self.validation_summary_writer.flush()\n",
    "                ################### $ ###################\n",
    "            if step % 1000 == 0:\n",
    "                print(\"Step = \", step)\n",
    "                \n",
    "        self.train_summary_writer.close()\n",
    "        self.validation_summary_writer.close()\n",
    "                \n",
    "    \n",
    "    def save(self):\n",
    "        self.saver.save(self.ses, save_path=self.save_path + \"/\" + self.name + '.ckpt')\n",
    "    \n",
    "    def load(self):\n",
    "        self.saver.restore(self.ses, save_path=self.save_path + \"/\" + self.name + '.ckpt')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'M2/Inference/layer1/kernel:0' shape=(784, 128) dtype=float32_ref>, <tf.Variable 'M2/Inference/layer1/bias:0' shape=(128,) dtype=float32_ref>]\n",
      "Step =  0\n",
      "Step =  1000\n",
      "Step =  2000\n",
      "Step =  3000\n",
      "Step =  4000\n",
      "Step =  5000\n",
      "Step =  6000\n",
      "Step =  7000\n",
      "Step =  8000\n",
      "Step =  9000\n",
      "Step =  10000\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(batch_size=64)\n",
    "M2 = Model(name=\"M2\", dataloader=dataloader)\n",
    "# show_graph(tf.get_default_graph())\n",
    "M2.train(init=True, steps=10000)\n",
    "# M2.save()\n",
    "# M1.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3745401188473625\n",
      "0.9507143064099162\n",
      "0.7319939418114051\n",
      "0.5986584841970366\n",
      "0.15601864044243652\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "def generator():\n",
    "    while True:\n",
    "        yield np.random.rand()\n",
    "        \n",
    "i = 0\n",
    "for x in generator():\n",
    "    print(x)\n",
    "    i += 1\n",
    "    if i == 5:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28, 28, 1), 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gzip, numpy\n",
    "import pickle\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, batch_size=32):\n",
    "        self.b = batch_size\n",
    "        # Load the dataset\n",
    "        with gzip.open('Dataset/mnist.pkl.gz', 'rb') as f:\n",
    "            u = pickle._Unpickler(f)\n",
    "            u.encoding = 'latin1'\n",
    "            train_set, val_set, _ = u.load()\n",
    "        \n",
    "        self.X_train, self.Y_train = train_set\n",
    "        self.X_val, self.Y_val = val_set\n",
    "        self.X_train = np.reshape(self.X_train, [-1, 28, 28, 1])\n",
    "        self.X_val = np.reshape(self.X_val, [-1, 28, 28, 1])\n",
    "     \n",
    "        self.n_train, self.n_val = len(self.X_train), len(self.X_val)\n",
    "        \n",
    "        self.train_dataset_initializer, self.train_dataset_batch = self.create_dataset(self.train_generator)\n",
    "        self.val_dataset_initializer, self.val_dataset_batch = self.create_dataset(self.val_generator)\n",
    "        \n",
    "    def train_generator(self):\n",
    "        for i in range(self.n_train):\n",
    "            yield self.X_train[i], self.Y_train[i]\n",
    "            \n",
    "    def val_generator(self):\n",
    "        for i in range(self.n_val):\n",
    "            yield self.X_val[i], self.Y_val[i]\n",
    "            \n",
    "    def create_dataset(self, generator):\n",
    "        output_shapes = (\n",
    "                tf.TensorShape([28, 28, 1]),\n",
    "                tf.TensorShape([])\n",
    "        )\n",
    "        dataset = tf.data.Dataset.from_generator(generator=generator,\n",
    "                                                 output_types=(tf.float32, tf.int64),\n",
    "                                                 output_shapes=output_shapes\n",
    "                                                )\n",
    "        \n",
    "        dataset_iterator = dataset.make_initializable_iterator()\n",
    "        dataset_initializer = dataset_iterator.initializer\n",
    "        dataset_batch = dataset_iterator.get_next()\n",
    "        return dataset_initializer, dataset_batch\n",
    "        \n",
    "        \n",
    "    \n",
    "tf.reset_default_graph()\n",
    "dataloader = DataLoader(batch_size=5)\n",
    "with tf.Session() as ses:\n",
    "    ses.run(dataloader.train_dataset_initializer)\n",
    "    x, y = ses.run(dataloader.train_dataset_batch)\n",
    "x.shape, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmenting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import rotate\n",
    "def augment(image):\n",
    "    augmented_images = [image >= 0.5]\n",
    "    for degree in [-20, -10, 10, 20]:\n",
    "        rotated_image = rotate(image, degree, reshape=False)\n",
    "        rotated_image_thresholded = rotated_image >= 0.5\n",
    "        \n",
    "        augmented_images.append(rotated_image_thresholded)\n",
    "    return augmented_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x299ae63a898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAC1RJREFUeJzt3W+o3XUdwPH3pzUnTQOHzZattJBIhGZcZmDEQqwVwvRB4h7Eguj2QCHBB8me6JNAor8PIlg1WlBakOYeSCkjUCGGV4m2WqXIyrWxqyzYCpqb+/Tg/ha3ef+cnfP7nd+5ft4vGPecc8+9v48H3/d3zv39zv1GZiKpnrf1PYCkfhi/VJTxS0UZv1SU8UtFGb9UlPFLRRm/VJTxS0W9fZwbuyTW5KWsHecmpVL+w795PU/HIPcdKf6I2Ap8F1gF/DAzH1rq/peylpvillE2KWkJ+3PfwPcd+ml/RKwCvgd8Brge2B4R1w/7/SSN1yiv+TcDL2Xmy5n5OvAIsK2dsSR1bZT4rwZemXf9SHPb/4mI6YiYiYiZM5weYXOS2jRK/Av9UuFN7w/OzF2ZOZWZU6tZM8LmJLVplPiPABvnXX8vcHS0cSSNyyjxPwdcFxHXRsQlwF3A3nbGktS1oQ/1ZebZiLgH+A1zh/p2Z+YfW5tMUqdGOs6fmU8AT7Q0i6Qx8vReqSjjl4oyfqko45eKMn6pKOOXijJ+qSjjl4oyfqko45eKMn6pKOOXijJ+qSjjl4oyfqko45eKMn6pKOOXijJ+qSjjl4oyfqko45eKMn6pKOOXijJ+qSjjl4oyfqko45eKMn6pqJFW6Y2Iw8Ap4A3gbGZOtTGUpO6NFH/jk5n5WgvfR9IY+bRfKmrU+BN4MiKej4jpNgaSNB6jPu2/OTOPRsR64KmI+HNmPj3/Ds0PhWmAS3nHiJuT1JaR9vyZebT5OAs8Bmxe4D67MnMqM6dWs2aUzUlq0dDxR8TaiLj8/GXgU8DBtgaT1K1RnvZfBTwWEee/z88y89etTCWpc0PHn5kvAx9pcRYt4jdHf9/3CJ349Hs29T1CaR7qk4oyfqko45eKMn6pKOOXijJ+qag23tUnDaXPQ5geZnTPL5Vl/FJRxi8VZfxSUcYvFWX8UlHGLxXlcf4VYJRj0m/VtwOPqu/HZRLOM3DPLxVl/FJRxi8VZfxSUcYvFWX8UlHGLxXlcf63uOWOJ3d9vHup7fe57eV0PdtS339c5wC455eKMn6pKOOXijJ+qSjjl4oyfqko45eKWvY4f0TsBm4DZjPzhua2dcDPgWuAw8CdmfnP7sbUsCb5WPqo5yB0eTx8Et5v37VB9vw/BrZecNv9wL7MvA7Y11yXtIIsG39mPg2cuODmbcCe5vIe4PaW55LUsWFf81+VmccAmo/r2xtJ0jh0fm5/REwD0wCX8o6uNydpQMPu+Y9HxAaA5uPsYnfMzF2ZOZWZU6tZM+TmJLVt2Pj3AjuayzuAx9sZR9K4LBt/RDwM/A74UEQciYgvAg8Bt0bEi8CtzXVJK8iyr/kzc/sin7ql5VnUgb7fzz+KCsfa++QZflJRxi8VZfxSUcYvFWX8UlHGLxVl/FJRxi8VZfxSUcYvFWX8UlHGLxVl/FJRxi8VZfxSUcYvFWX8UlHGLxVl/FJRxi8VZfxSUcYvFdX5cl2abKP+ae8+l9HWaNzzS0UZv1SU8UtFGb9UlPFLRRm/VJTxS0Ute5w/InYDtwGzmXlDc9uDwJeAV5u77czMJ7oaUivXUucBeA5AvwbZ8/8Y2LrA7d/OzE3NP8OXVphl48/Mp4ETY5hF0hiN8pr/noj4Q0TsjogrWptI0lgMG//3gQ8Cm4BjwDcXu2NETEfETETMnOH0kJuT1Lah4s/M45n5RmaeA34AbF7ivrsycyozp1azZtg5JbVsqPgjYsO8q3cAB9sZR9K4DHKo72FgC3BlRBwBHgC2RMQmIIHDwJc7nFFSByIzx7axd8a6vCluGdv21L3l3s8/Cs8DuHj7cx8n80QMcl/P8JOKMn6pKOOXijJ+qSjjl4oyfqko/3S3RjLqn/5Wf9zzS0UZv1SU8UtFGb9UlPFLRRm/VJTxS0V5nF8Ty+W/u+WeXyrK+KWijF8qyvilooxfKsr4paKMXyrK4/xj4HvaNYnc80tFGb9UlPFLRRm/VJTxS0UZv1SU8UtFLXucPyI2Aj8B3g2cA3Zl5ncjYh3wc+Aa4DBwZ2b+c5RhPB6u+Xy/frcG2fOfBe7LzA8DHwPujojrgfuBfZl5HbCvuS5phVg2/sw8lpkvNJdPAYeAq4FtwJ7mbnuA27saUlL7Luo1f0RcA9wI7AeuysxjMPcDAljf9nCSujNw/BFxGfBL4N7MPHkRXzcdETMRMXOG08PMKKkDA8UfEauZC/+nmfloc/PxiNjQfH4DMLvQ12bmrsycysyp1axpY2ZJLVg2/ogI4EfAocz81rxP7QV2NJd3AI+3P56krkRmLn2HiI8DzwAHmDvUB7CTudf9vwDeB/wd+Fxmnljqe70z1uVNccvQw3oocOXxcN147c99nMwTMch9lz3On5nPAot9s+FLltQrz/CTijJ+qSjjl4oyfqko45eKMn6pqBX1p7sn9ZjxqOcfTOp/l97a3PNLRRm/VJTxS0UZv1SU8UtFGb9UlPFLRa2o4/yTyuP0Wonc80tFGb9UlPFLRRm/VJTxS0UZv1SU8UtFGb9UlPFLRRm/VJTxS0UZv1SU8UtFGb9UlPFLRS0bf0RsjIjfRsShiPhjRHyluf3BiPhHRPy++ffZ7seV1JZB/pjHWeC+zHwhIi4Hno+Ip5rPfTszv9HdeJK6smz8mXkMONZcPhURh4Crux5MUrcu6jV/RFwD3Ajsb266JyL+EBG7I+KKRb5mOiJmImLmDKdHGlZSewaOPyIuA34J3JuZJ4HvAx8ENjH3zOCbC31dZu7KzKnMnFrNmhZGltSGgeKPiNXMhf/TzHwUIDOPZ+YbmXkO+AGwubsxJbVtkN/2B/Aj4FBmfmve7Rvm3e0O4GD740nqyiC/7b8Z+DxwICLOr0W9E9geEZuABA4DX+5kQkmdGOS3/c8CscCnnmh/HEnj4hl+UlHGLxVl/FJRxi8VZfxSUcYvFWX8UlHGLxVl/FJRxi8VZfxSUcYvFWX8UlHGLxUVmTm+jUW8Cvxt3k1XAq+NbYCLM6mzTepc4GzDanO292fmuwa541jjf9PGI2Yyc6q3AZYwqbNN6lzgbMPqazaf9ktFGb9UVN/x7+p5+0uZ1NkmdS5wtmH1Mluvr/kl9afvPb+knvQSf0RsjYi/RMRLEXF/HzMsJiIOR8SBZuXhmZ5n2R0RsxFxcN5t6yLiqYh4sfm44DJpPc02ESs3L7GydK+P3aSteD32p/0RsQr4K3ArcAR4DtiemX8a6yCLiIjDwFRm9n5MOCI+AfwL+Elm3tDc9nXgRGY+1PzgvCIzvzohsz0I/KvvlZubBWU2zF9ZGrgd+AI9PnZLzHUnPTxufez5NwMvZebLmfk68AiwrYc5Jl5mPg2cuODmbcCe5vIe5v7nGbtFZpsImXksM19oLp8Czq8s3etjt8Rcvegj/quBV+ZdP8JkLfmdwJMR8XxETPc9zAKuapZNP798+vqe57nQsis3j9MFK0tPzGM3zIrXbesj/oVW/5mkQw43Z+ZHgc8AdzdPbzWYgVZuHpcFVpaeCMOueN22PuI/Amycd/29wNEe5lhQZh5tPs4CjzF5qw8fP79IavNxtud5/meSVm5eaGVpJuCxm6QVr/uI/znguoi4NiIuAe4C9vYwx5tExNrmFzFExFrgU0ze6sN7gR3N5R3A4z3O8n8mZeXmxVaWpufHbtJWvO7lJJ/mUMZ3gFXA7sz82tiHWEBEfIC5vT3MLWL6sz5ni4iHgS3MvevrOPAA8CvgF8D7gL8Dn8vMsf/ibZHZtjD31PV/Kzeff4095tk+DjwDHADONTfvZO71dW+P3RJzbaeHx80z/KSiPMNPKsr4paKMXyrK+KWijF8qyvilooxfKsr4paL+C/tFX5DBVQ0+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x299e7a6fef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gzip, numpy\n",
    "import pickle\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, batch_size=32):\n",
    "        self.b = batch_size\n",
    "        # Load the dataset\n",
    "        with gzip.open('Dataset/mnist.pkl.gz', 'rb') as f:\n",
    "            u = pickle._Unpickler(f)\n",
    "            u.encoding = 'latin1'\n",
    "            train_set, val_set, _ = u.load()\n",
    "        \n",
    "        self.X_train, self.Y_train = train_set\n",
    "        self.X_val, self.Y_val = val_set\n",
    "        self.X_train = np.reshape(self.X_train, [-1, 28, 28, 1])\n",
    "        self.X_val = np.reshape(self.X_val, [-1, 28, 28, 1])\n",
    "     \n",
    "        self.n_train, self.n_val = len(self.X_train), len(self.X_val)\n",
    "        \n",
    "        self.train_dataset_initializer, self.train_dataset_batch = self.create_dataset(self.train_generator)\n",
    "        self.val_dataset_initializer, self.val_dataset_batch = self.create_dataset(self.val_generator)\n",
    "        \n",
    "    def train_generator(self):\n",
    "        for i in range(self.n_train):\n",
    "            for x in augment(self.X_train[i]):\n",
    "                yield x, self.Y_train[i]\n",
    "            \n",
    "    def val_generator(self):\n",
    "        for i in range(self.n_train):\n",
    "            yield self.X_train[i], self.Y_train[i]\n",
    "            \n",
    "    def create_dataset(self, generator):\n",
    "        output_shapes = (\n",
    "                tf.TensorShape([28, 28, 1]),\n",
    "                tf.TensorShape([])\n",
    "        )\n",
    "        dataset = tf.data.Dataset.from_generator(generator=generator,\n",
    "                                                 output_types=(tf.float32, tf.int64),\n",
    "                                                 output_shapes=output_shapes\n",
    "                                                )\n",
    "        \n",
    "        dataset_iterator = dataset.make_initializable_iterator()\n",
    "        dataset_initializer = dataset_iterator.initializer\n",
    "        dataset_batch = dataset_iterator.get_next()\n",
    "        return dataset_initializer, dataset_batch\n",
    "        \n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "tf.reset_default_graph()\n",
    "dataloader = DataLoader(batch_size=5)\n",
    "with tf.Session() as ses:\n",
    "    ses.run(dataloader.train_dataset_initializer)\n",
    "    x, y = ses.run(dataloader.train_dataset_batch)\n",
    "    plt.figure()\n",
    "    plt.imshow(x[:, :, 0])\n",
    "    x, y = ses.run(dataloader.train_dataset_batch)\n",
    "    \n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(x[:, :, 0])\n",
    "    \n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle, Batch, Prefetch, Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 28, 28, 1) (64,)\n",
      "(64, 28, 28, 1) int64\n"
     ]
    }
   ],
   "source": [
    "import gzip, numpy\n",
    "import pickle\n",
    "\n",
    "def augment(image):\n",
    "    augmented_images = [image >= 0.5]\n",
    "    for degree in [-20, -10, 10, 20]:\n",
    "        rotated_image = rotate(image, degree, reshape=False)\n",
    "        rotated_image_thresholded = rotated_image >= 0.5\n",
    "        \n",
    "        augmented_images.append(rotated_image_thresholded)\n",
    "    return augmented_images\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, batch_size=32):\n",
    "        self.b = batch_size\n",
    "        # Load the dataset\n",
    "        with gzip.open('Dataset/mnist.pkl.gz', 'rb') as f:\n",
    "            u = pickle._Unpickler(f)\n",
    "            u.encoding = 'latin1'\n",
    "            train_set, val_set, _ = u.load()\n",
    "        \n",
    "        self.X_train, self.Y_train = train_set\n",
    "        self.X_val, self.Y_val = val_set\n",
    "        self.X_train = np.reshape(self.X_train, [-1, 28, 28, 1])\n",
    "        self.X_val = np.reshape(self.X_val, [-1, 28, 28, 1])\n",
    "     \n",
    "        self.n_train, self.n_val = len(self.X_train), len(self.X_val)\n",
    "        \n",
    "#         tf.set_random_seed(42)\n",
    "        self.train_dataset_initializer, self.train_dataset_batch = self.create_dataset(self.train_generator)\n",
    "        self.val_dataset_initializer, self.val_dataset_batch = self.create_dataset(self.val_generator)\n",
    "        \n",
    "    def train_generator(self):\n",
    "        for i in range(self.n_train):\n",
    "            for x in augment(self.X_train[i]):\n",
    "                yield x, self.Y_train[i]\n",
    "            \n",
    "    def val_generator(self):\n",
    "        for i in range(self.n_train):\n",
    "            yield self.X_train[i], self.Y_train[i]\n",
    "            \n",
    "    def create_dataset(self, generator):\n",
    "        output_shapes = (\n",
    "                tf.TensorShape([28, 28, 1]),\n",
    "                tf.TensorShape([])\n",
    "        )\n",
    "        dataset = tf.data.Dataset.from_generator(generator=generator,\n",
    "                                                 output_types=(tf.float32, tf.int64),\n",
    "                                                 output_shapes=output_shapes\n",
    "                                                )\n",
    "        \n",
    "        \n",
    "        dataset = dataset.prefetch(self.b * 5)\n",
    "        dataset = dataset.shuffle(buffer_size=self.b * 5)\n",
    "        dataset = dataset.batch(self.b)\n",
    "        dataset_iterator = dataset.make_initializable_iterator()\n",
    "        dataset_initializer = dataset_iterator.initializer\n",
    "        dataset_batch = dataset_iterator.get_next()\n",
    "        return dataset_initializer, dataset_batch\n",
    "\n",
    "        \n",
    "   \n",
    "\n",
    "tf.reset_default_graph()\n",
    "dataloader = DataLoader(batch_size=64)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# with tf.Session() as ses:\n",
    "#     ses.run(dataloader.train_dataset_initializer)\n",
    "#     x, y = ses.run(dataloader.train_dataset_batch)\n",
    "#     plt.figure()\n",
    "#     plt.imshow(x[:, :, 0])\n",
    "#     x, y = ses.run(dataloader.train_dataset_batch)\n",
    "    \n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.imshow(x[:, :, 0])\n",
    "    \n",
    "    \n",
    "with tf.Session() as ses:\n",
    "    ses.run(dataloader.train_dataset_initializer)\n",
    "    x, y = ses.run(dataloader.train_dataset_batch)\n",
    "    print(x.shape, y.shape)\n",
    "    print(x.shape, y.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens when we reach to the end of dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfRangeError",
     "evalue": "End of sequence\n\t [[node IteratorGetNext_1 (defined at <ipython-input-19-0eda62809dbf>:58)  = IteratorGetNext[output_shapes=[[?,28,28,1], [?]], output_types=[DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](IteratorV2_1)]]\n\nCaused by op 'IteratorGetNext_1', defined at:\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 112, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\asyncio\\base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\asyncio\\base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\asyncio\\events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 102, in _handle_events\n    handler_func(fileobj, events)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2705, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2809, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2869, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-20-036955c32f5a>\", line 2, in <module>\n    dataloader = DataLoader(batch_size=64)\n  File \"<ipython-input-19-0eda62809dbf>\", line 31, in __init__\n    self.val_dataset_initializer, self.val_dataset_batch = self.create_dataset(self.val_generator)\n  File \"<ipython-input-19-0eda62809dbf>\", line 58, in create_dataset\n    dataset_batch = dataset_iterator.get_next()\n  File \"C:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 421, in get_next\n    name=name)), self._output_types,\n  File \"C:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\", line 2108, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"C:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nOutOfRangeError (see above for traceback): End of sequence\n\t [[node IteratorGetNext_1 (defined at <ipython-input-19-0eda62809dbf>:58)  = IteratorGetNext[output_shapes=[[?,28,28,1], [?]], output_types=[DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](IteratorV2_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence\n\t [[{{node IteratorGetNext_1}} = IteratorGetNext[output_shapes=[[?,28,28,1], [?]], output_types=[DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](IteratorV2_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-036955c32f5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataset_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_dataset_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# OutOfRangeError (see above for traceback): End of sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence\n\t [[node IteratorGetNext_1 (defined at <ipython-input-19-0eda62809dbf>:58)  = IteratorGetNext[output_shapes=[[?,28,28,1], [?]], output_types=[DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](IteratorV2_1)]]\n\nCaused by op 'IteratorGetNext_1', defined at:\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 112, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\asyncio\\base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\asyncio\\base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\asyncio\\events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 102, in _handle_events\n    handler_func(fileobj, events)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2705, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2809, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\ehsan\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2869, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-20-036955c32f5a>\", line 2, in <module>\n    dataloader = DataLoader(batch_size=64)\n  File \"<ipython-input-19-0eda62809dbf>\", line 31, in __init__\n    self.val_dataset_initializer, self.val_dataset_batch = self.create_dataset(self.val_generator)\n  File \"<ipython-input-19-0eda62809dbf>\", line 58, in create_dataset\n    dataset_batch = dataset_iterator.get_next()\n  File \"C:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\", line 421, in get_next\n    name=name)), self._output_types,\n  File \"C:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\", line 2108, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"C:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nOutOfRangeError (see above for traceback): End of sequence\n\t [[node IteratorGetNext_1 (defined at <ipython-input-19-0eda62809dbf>:58)  = IteratorGetNext[output_shapes=[[?,28,28,1], [?]], output_types=[DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](IteratorV2_1)]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "dataloader = DataLoader(batch_size=64)\n",
    "# with tf.Session() as ses:\n",
    "#     ses.run(dataloader.val_dataset_initializer)\n",
    "#     while True:\n",
    "#         x, y = ses.run(dataloader.val_dataset_batch) # OutOfRangeError (see above for traceback): End of sequence\n",
    "    \n",
    "\n",
    "with tf.Session() as ses:\n",
    "    ses.run(dataloader.val_dataset_initializer)\n",
    "    while True:\n",
    "        try:\n",
    "            x, y = ses.run(dataloader.val_dataset_batch) # OutOfRangeError (see above for traceback): End of sequence\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            ses.run(dataloader.val_dataset_initializer)\n",
    "            print(\"ReInitializing dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, numpy\n",
    "import pickle\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "\n",
    "def augment(image):\n",
    "    augmented_images = [image >= 0.5]\n",
    "    for degree in [-20, -10, 10, 20]:\n",
    "        rotated_image = rotate(image, degree, reshape=False)\n",
    "        rotated_image_thresholded = rotated_image >= 0.5\n",
    "        \n",
    "        augmented_images.append(rotated_image_thresholded)\n",
    "    return augmented_images\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, batch_size=32):\n",
    "        self.b = batch_size\n",
    "        # Load the dataset\n",
    "        with gzip.open('Dataset/mnist.pkl.gz', 'rb') as f:\n",
    "            u = pickle._Unpickler(f)\n",
    "            u.encoding = 'latin1'\n",
    "            train_set, val_set, _ = u.load()\n",
    "        \n",
    "        self.X_train, self.Y_train = train_set\n",
    "        self.X_val, self.Y_val = val_set\n",
    "        self.X_train = np.reshape(self.X_train, [-1, 28, 28, 1])\n",
    "        self.X_val = np.reshape(self.X_val, [-1, 28, 28, 1])\n",
    "     \n",
    "        self.n_train, self.n_val = len(self.X_train), len(self.X_val)\n",
    "        \n",
    "#         tf.set_random_seed(42)\n",
    "        self.train_dataset_initializer, self.train_dataset_batch = self.create_dataset(self.train_generator)\n",
    "        self.val_dataset_initializer, self.val_dataset_batch = self.create_dataset(self.val_generator)\n",
    "        \n",
    "    def train_generator(self):\n",
    "        for i in range(self.n_train):\n",
    "            for x in augment(self.X_train[i]):\n",
    "                yield x, self.Y_train[i]\n",
    "            \n",
    "    def val_generator(self):\n",
    "        for i in range(self.n_train):\n",
    "            yield self.X_train[i], self.Y_train[i]\n",
    "            \n",
    "    def create_dataset(self, generator):\n",
    "        output_shapes = (\n",
    "                tf.TensorShape([28, 28, 1]),\n",
    "                tf.TensorShape([])\n",
    "        )\n",
    "        dataset = tf.data.Dataset.from_generator(generator=generator,\n",
    "                                                 output_types=(tf.float32, tf.int64),\n",
    "                                                 output_shapes=output_shapes\n",
    "                                                )\n",
    "        \n",
    "        \n",
    "        dataset = dataset.prefetch(self.b * 5)\n",
    "        dataset = dataset.shuffle(buffer_size=self.b * 5)\n",
    "        dataset = dataset.batch(self.b)\n",
    "        dataset_iterator = dataset.make_initializable_iterator()\n",
    "        dataset_initializer = dataset_iterator.initializer\n",
    "        dataset_batch = dataset_iterator.get_next()\n",
    "        return dataset_initializer, dataset_batch        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, name='Model'):\n",
    "        self.name = name\n",
    "        \n",
    "        self.save_path = os.path.join(\"SavedModels/\", self.name)\n",
    "        try:\n",
    "            os.mkdir(self.save_path)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        init_time = datetime.datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "        self.log_path = os.path.join(\"log/\" + self.name + \"-run-\" + init_time)\n",
    "        \n",
    "        self.create_graph()\n",
    "        self.ses = tf.Session()\n",
    "        \n",
    "    \n",
    "    def create_graph(self):\n",
    "        tf.reset_default_graph()\n",
    "        tf.set_random_seed(42)\n",
    "        with tf.variable_scope(self.name):\n",
    "            with tf.variable_scope(\"DataLoader\"):\n",
    "                self.dataloader = DataLoader(batch_size=64)\n",
    "                self.x_train, self.y_train = self.dataloader.train_dataset_batch\n",
    "                self.x_val, self.y_val = self.dataloader.val_dataset_batch\n",
    "                self.train_dataset_initializer = self.dataloader.train_dataset_initializer\n",
    "                self.val_dataset_initializer = self.dataloader.val_dataset_initializer\n",
    "                \n",
    "            with tf.variable_scope(\"Placeholders\"):\n",
    "#                 self.x_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28, 1]) # dataloader must provide the shape\n",
    "#                 self.y_placeholder = tf.placeholder(dtype=tf.int64, shape=[None])\n",
    "                self.x_placeholder = tf.placeholder_with_default(self.x_train,\n",
    "                                                                shape=[None, 28, 28, 1],\n",
    "                                                                name='x_placeholder')\n",
    "                self.y_placeholder = tf.placeholder_with_default(self.y_train,\n",
    "                                                                shape=[None],\n",
    "                                                                name='y_placeholder')\n",
    "    \n",
    "            \n",
    "            with tf.variable_scope(\"Inference\"):\n",
    "                x_flattened = tf.layers.flatten(self.x_placeholder, name='x_flattened')\n",
    "                h1 = tf.layers.dense(inputs=x_flattened, units=128, activation=tf.nn.tanh, name='layer1')\n",
    "                self.logits = tf.layers.dense(inputs=h1, units=10, name='y_hat')\n",
    "                self.y_hat_probs = tf.nn.softmax(self.logits, name='y_hat_probs')\n",
    "                \n",
    "            with tf.variable_scope(\"Optimization\"):\n",
    "                batch_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y_placeholder,\n",
    "                                                                            logits=self.logits,\n",
    "                                                                            name='batch_loss')\n",
    "                self.loss = tf.reduce_mean(batch_loss, name='loss')\n",
    "                batch_accuracy = tf.cast(\n",
    "                    tf.equal(\n",
    "                        tf.argmax(self.y_hat_probs, axis=1),\n",
    "                        self.y_placeholder\n",
    "                    ), dtype=tf.float32)\n",
    "                self.accuracy = tf.reduce_mean(batch_accuracy)\n",
    "                self.global_step = tf.train.get_or_create_global_step()\n",
    "                learning_rate = tf.train.exponential_decay(learning_rate=1e-2, # 1e-3\n",
    "                                                           decay_steps=2000,\n",
    "                                                           decay_rate=0.5,\n",
    "                                                           global_step=self.global_step,\n",
    "                                                           name='learning_rate')\n",
    "                optimizer = tf.train.AdagradOptimizer(learning_rate)\n",
    "                self.train_operation = optimizer.minimize(self.loss, global_step=self.global_step)\n",
    "                \n",
    "                ################### $ ###################\n",
    "                layer1_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.name + \"/Inference/layer1/\")\n",
    "                layer1_gradients = tf.gradients(self.loss, layer1_variables)\n",
    "                layer1_gradient_norm = tf.norm(layer1_gradients[0], name='layer1_gradient_norm')\n",
    "                ################### $ ###################\n",
    "                \n",
    "            with tf.variable_scope(\"Init_save\"):\n",
    "                self.variable_initializer = tf.global_variables_initializer()\n",
    "                self.saver = tf.train.Saver()\n",
    "                \n",
    "            with tf.variable_scope(\"Summary\"):\n",
    "                self.loss_summary = tf.summary.scalar(name=\"Cross_Entropy_Loss\", tensor=self.loss)\n",
    "                self.accuracy_summary = tf.summary.scalar(name=\"Accuracy\", tensor=self.accuracy)\n",
    "                self.eval_summaries = tf.summary.merge([self.loss_summary, self.accuracy_summary])\n",
    "                \n",
    "                ################### $ ###################\n",
    "                self.layer1_gradient_norm_summary = tf.summary.scalar(name=\"layer1_gradient_norm\", \n",
    "                                                                      tensor=layer1_gradient_norm)\n",
    "                self.layer1_kernel_histogram_summary = tf.summary.histogram(name='layer1_kernel_histogram', \n",
    "                                                                            values=layer1_variables[0])\n",
    "                self.learning_rate_summary = tf.summary.scalar(name='learning_rate', tensor=learning_rate)\n",
    "                \n",
    "                self.input_image_summary = tf.summary.image(name='input_image', tensor=self.x_placeholder, max_outputs=3)\n",
    "                neuron_activity = tf.reshape(tf.transpose(layer1_variables[0]), [128, 28, 28, 1], name='neuron_activity')\n",
    "                self.neuron_activity_summary = tf.summary.image(name='neuron_activity',\n",
    "                                                                tensor=neuron_activity,\n",
    "                                                                max_outputs=3)\n",
    "                self.merged_summaries = tf.summary.merge_all()\n",
    "                ################### $ ###################\n",
    "                \n",
    "                self.train_summary_writer = tf.summary.FileWriter(self.log_path + \"train\", tf.get_default_graph())\n",
    "                self.validation_summary_writer = tf.summary.FileWriter(self.log_path + \"validation\")\n",
    "    \n",
    "                \n",
    "                \n",
    "                \n",
    "    def train(self, init=True, steps=10000):\n",
    "        if init:\n",
    "            self.ses.run(self.variable_initializer)\n",
    "        step = 0\n",
    "        ################### $ ###################\n",
    "        self.ses.run(self.val_dataset_initializer)\n",
    "        self.ses.run(self.train_dataset_initializer)\n",
    "        ################### $ ###################\n",
    "\n",
    "        while step < steps:\n",
    "            # We don't need feed_dict anymore (for training)\n",
    "            try:\n",
    "                train_summary, step, _ = self.ses.run([self.merged_summaries, self.global_step, self.train_operation])\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                print(\"Reinitializing Train dataset\")\n",
    "                self.ses.run(self.train_dataset_initializer)\n",
    "                \n",
    "            try:\n",
    "                x_val, y_val = self.ses.run([self.x_val, self.y_val])\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                self.ses.run(self.val_dataset_initializer)\n",
    "                print(\"Reinitializing Validation dataset\")\n",
    "\n",
    "                \n",
    "            val_loss_summary = self.ses.run(self.eval_summaries,\n",
    "                                            feed_dict={self.x_placeholder: x_val, self.y_placeholder: y_val})\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                try:\n",
    "                    self.train_summary_writer.add_summary(train_summary, step)\n",
    "                    self.validation_summary_writer.add_summary(val_loss_summary, step)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            if step % 1000 == 0:\n",
    "                print(\"Step = \", step)\n",
    "                \n",
    "        self.train_summary_writer.close()\n",
    "        self.validation_summary_writer.close()\n",
    "                \n",
    "    \n",
    "    def save(self):\n",
    "        self.saver.save(self.ses, save_path=self.save_path + \"/\" + self.name + '.ckpt')\n",
    "    \n",
    "    def load(self):\n",
    "        self.saver.restore(self.ses, save_path=self.save_path + \"/\" + self.name + '.ckpt')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step =  0\n",
      "Reinitializing Validation dataset\n",
      "Step =  1000\n",
      "Reinitializing Validation dataset\n",
      "Step =  2000\n",
      "Reinitializing Validation dataset\n",
      "Step =  3000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c17f1c1299d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mM3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"M3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# show_graph(tf.get_default_graph())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mM3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# M3.save()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# M3.load()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-8f0cb6654429>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, init, steps)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;31m# We don't need feed_dict anymore (for training)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0mtrain_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerged_summaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_operation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reinitializing Train dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Ehsan\\AppData\\Roaming\\Python\\Python35\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "M3 = Model(name=\"M3\")\n",
    "# show_graph(tf.get_default_graph())\n",
    "M3.train(init=True, steps=10000)\n",
    "# M3.save()\n",
    "# M3.load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
